{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f81c476d",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4d1b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7342c31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load data \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/Combined_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "data = pd.read_csv('data/Combined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert statement col to string\n",
    "data['statement'] = data['statement'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288d215",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def summarize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarizes the given DataFrame by providing key statistics for each column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to summarize.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A summary DataFrame containing statistics for each column.\n",
    "    \"\"\"\n",
    "    summary = pd.DataFrame({\n",
    "        'Data Type': df.dtypes,\n",
    "        'Non-Null Count': df.notnull().sum(),\n",
    "        'Unique Values': df.nunique(),\n",
    "        'Mean': df.select_dtypes(include=['number']).mean(),\n",
    "        'Median': df.select_dtypes(include=['number']).median(),\n",
    "        'Min': df.select_dtypes(include=['number']).min(),\n",
    "        'Max': df.select_dtypes(include=['number']).max(),\n",
    "        'Std Dev': df.select_dtypes(include=['number']).std()\n",
    "    })\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "a. Load & Inspect\n",
    "\n",
    "Check for missing or duplicate Statement entries.\n",
    "\n",
    "Examine class distribution and total record count.\n",
    "\n",
    "Compute text length statistics (tokens per post, characters).\n",
    "\"\"\"\n",
    "missing_data = data['statement'].isnull().sum()\n",
    "duplicate_data = data['statement'].duplicated().sum()\n",
    "class_distribution = data['status'].value_counts()\n",
    "total_records = len(data)\n",
    "\n",
    "print(\"Missing data:\")\n",
    "print(missing_data)\n",
    "\n",
    "print(\"Duplicate data:\")\n",
    "print(duplicate_data)\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(class_distribution)\n",
    "\n",
    "print(\"Total records:\")\n",
    "print(total_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "b. Cleaning\n",
    "\n",
    "Lowercasing, remove URLs, emojis, punctuation.\n",
    "\n",
    "Expand contractions (e.g., “don’t → do not”).\n",
    "\n",
    "Optional: lemmatize for LDA pipeline.\n",
    "\"\"\"\n",
    "\n",
    "def clean_text(text: str, lemmatize: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the input text by lowercasing, removing URLs, emojis, punctuation,\n",
    "    expanding contractions, and optionally lemmatizing.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to clean.\n",
    "    lemmatize (bool): Whether to lemmatize the text.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lowercase\n",
    "    # print(\"text: \", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize if required\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "data['cleaned_statement'] = data['statement'].apply(lambda x: clean_text(x, lemmatize=True))\n",
    "print(data[['statement', 'cleaned_statement']].head())  \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae89549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"c. Exploration\n",
    "\n",
    "Bar chart of label frequencies (to reveal imbalance).\n",
    "\n",
    "Word clouds / top n-grams per category.\n",
    "\n",
    "TF-IDF keyword comparison across classes.\"\"\"\n",
    "\n",
    "# Bar chart of label frequencies\n",
    "label_counts = data['status'].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.title('Label Frequencies')\n",
    "plt.xlabel('Mental Health Status')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
